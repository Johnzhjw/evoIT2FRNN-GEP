{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\toolkits.win\\Anaconda3-2021.05\\envs\\tf-cpu-1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-2021.05\\envs\\tf-cpu-1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-2021.05\\envs\\tf-cpu-1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-2021.05\\envs\\tf-cpu-1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-2021.05\\envs\\tf-cpu-1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-2021.05\\envs\\tf-cpu-1.13\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-77de2930a1c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExponentialSmoothing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleExpSmoothing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHolt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "sns.set()\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, \\\n",
    "SimpleExpSmoothing, Holt\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(filename='../dataset/Stk.0941.HK.all.csv'):\n",
    "    df = pd.read_csv(filename)\n",
    "    cols = list(df)\n",
    "    #print(cols)\n",
    "    tmp=filename.index('Stk')\n",
    "    #print(tmp)\n",
    "    tmp+=len('Stk.')\n",
    "    tmpstr=filename[tmp:tmp+len('0941.HK.')]+'Adjusted'\n",
    "    tmpstr_close=filename[tmp:tmp+len('0941.HK.')]+'Close'\n",
    "    cols.insert(5, cols.pop(cols.index(tmpstr)))\n",
    "    cols.insert(1, cols.pop(cols.index(tmpstr_close)))\n",
    "    #print(cols)\n",
    "    #print(cols[0:2])\n",
    "    df = df.loc[:, cols]\n",
    "    df_all = df\n",
    "    ###\n",
    "    df = df.loc[:, cols[0:2]]\n",
    "    df_all = df_all.loc[:, cols[0:2]]\n",
    "    #print(df.iloc[0:1481,0])\n",
    "    num_train = 1480\n",
    "    df = df.iloc[0:num_train]\n",
    "    #print(df)\n",
    "    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "    date_all_ori = pd.to_datetime(df_all.iloc[:, 0]).tolist()\n",
    "    #print(min(df.iloc[:, 4]))\n",
    "    #print(max(df.iloc[:, 4]))\n",
    "    #print(df.head())\n",
    "    df.head()\n",
    "    \n",
    "    return df, df_all, date_ori, date_all_ori, num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm(df, df_all):\n",
    "    allmean = [np.mean(df.iloc[:, i+1]).astype('float32') for i in range(df.shape[1]-1)]\n",
    "    allstd  = [np.std(df.iloc[:, i+1]).astype('float32') for i in range(df.shape[1]-1)]\n",
    "    #print(allmean)\n",
    "    #print(allstd)\n",
    "    #print(allmean[0])\n",
    "    #print(allstd[0])\n",
    "    df_log = [(df.iloc[:, i+1].astype('float32')-np.mean(df.iloc[:, i+1]).astype('float32'))/np.std(df.iloc[:, i+1]).astype('float32') for i in range(df.shape[1]-1)]\n",
    "    #minmax = MinMaxScaler().fit(df.iloc[:, 1:].astype('float32'))\n",
    "    #df_log = minmax.transform(df.iloc[:, 1:].astype('float32'))\n",
    "    df_log = pd.DataFrame(df_log)\n",
    "    df_log = pd.DataFrame(df_log.values.T, index=df_log.columns, columns=df_log.index)\n",
    "    #print(df_log.head())\n",
    "    #print(df_log.shape[0])\n",
    "    #print(df_log.iloc[0,0])\n",
    "    #\n",
    "    df_all_log = [(df_all.iloc[:, i+1].astype('float32')-np.mean(df.iloc[:, i+1]).astype('float32'))/np.std(df.iloc[:, i+1]).astype('float32') for i in range(df.shape[1]-1)]\n",
    "    #minmax = MinMaxScaler().fit(df.iloc[:, 1:].astype('float32'))\n",
    "    #df_log = minmax.transform(df.iloc[:, 1:].astype('float32'))\n",
    "    df_all_log = pd.DataFrame(df_all_log)\n",
    "    df_all_log = pd.DataFrame(df_all_log.values.T, index=df_all_log.columns, columns=df_all_log.index)\n",
    "    #print(df_all_log.head())\n",
    "    #print(df_all_log.shape[0])\n",
    "    #print(df_all_log.iloc[0,0])\n",
    "    \n",
    "    return df_log, df_all_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_paras(num_layers, df_log, size_layer, dropout_rate):\n",
    "    modelnn = []\n",
    "    sess = []\n",
    "    \n",
    "    return modelnn, sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(modelName, df_log, modelnn):\n",
    "    #if modelName.find('exp') >= 0:\n",
    "    #    modelnn = SimpleExpSmoothing(np.asarray(df_log)).fit(smoothing_level=0.6,optimized=False)\n",
    "    #    print(modelnn)\n",
    "    #elif modelName.find('Holt-winters') >= 0:\n",
    "    #    modelnn = ExponentialSmoothing(np.asarray(df_log), seasonal_periods=7 ,trend='add', seasonal='add',).fit()\n",
    "    #    \n",
    "    return #modelnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(df_all_log, date_all_ori, future_day, timestamp, num_layers, size_layer):\n",
    "    df_log = df_all_log\n",
    "    date_ori = date_all_ori\n",
    "    #print(df_log.shape[0])\n",
    "    output_predict = np.zeros((df_log.shape[0] + future_day, df_log.shape[1]))\n",
    "    output_predict[0] = df_log.iloc[0]\n",
    "    #print(output_predict)\n",
    "    #print(df_log)\n",
    "    #print(output_predict.shape[0])\n",
    "    upper_b = df_log.shape[0]\n",
    "    num = df_log.shape[0]\n",
    "    if modelName.find('naive') >= 0:\n",
    "        output_predict[1:num] = df_log.iloc[0:num-1]\n",
    "        output_predict[num:] = df_log.iloc[num-1]\n",
    "    elif modelName.find('avg') >= 0:\n",
    "        for i in range(num-1):\n",
    "            output_predict[i+1] = np.mean(df_log.iloc[0:i+1])\n",
    "        output_predict[num:] = output_predict[num-1]\n",
    "    elif modelName.find('moving_window') >= 0:\n",
    "        output_predict[1:num] = df_log.rolling(3,min_periods=1).mean().iloc[0:num-1]\n",
    "        output_predict[num:] = df_log.iloc[num-1]\n",
    "    elif modelName.find('exp') >= 0:\n",
    "        output_predict[1:num_train] = df_log.iloc[1:num_train]\n",
    "        for iii in range(output_predict.shape[0]-num_train):\n",
    "            modelnn = SimpleExpSmoothing(np.asarray(df_log[0:num_train+iii])).fit(smoothing_level=0.6,optimized=False)\n",
    "            output_predict[num_train+iii] = modelnn.forecast(1)\n",
    "    elif modelName.find('Holt-winters') >= 0:\n",
    "        output_predict[1:num_train] = df_log.iloc[1:num_train]\n",
    "        for iii in range(output_predict.shape[0]-num_train):\n",
    "            modelnn = ExponentialSmoothing(np.asarray(df_log[0:num_train+iii]), seasonal_periods=7 ,trend='add', seasonal='add',).fit()\n",
    "            output_predict[num_train+iii] = modelnn.forecast(1)\n",
    "        \n",
    "    #print(upper_b)\n",
    "    #df_log.loc[df_log.shape[0]] = out_logits[-1]\n",
    "    date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "    \n",
    "    return output_predict, upper_b, df_log, date_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_predict_more(output_predict, upper_b, df_log, date_ori):\n",
    "    #print(output_predict[upper_b + 1 : df_log.shape[0] + 1])\n",
    "    for i in range(future_day - 1):\n",
    "        #out_logits, last_state = sess.run(\n",
    "        #    [modelnn.logits, modelnn.last_state],\n",
    "        #    feed_dict = {\n",
    "        #        modelnn.X: np.expand_dims(df_log.iloc[-timestamp:], axis = 0),\n",
    "        #        modelnn.hidden_layer: init_value,\n",
    "        #    },\n",
    "        #)\n",
    "        #init_value = last_state\n",
    "        #output_predict[df_log.shape[0]] = out_logits[-1]\n",
    "        #df_log.loc[df_log.shape[0]] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "        \n",
    "    return df_log, date_ori, output_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm_data(df_log, date_ori, output_predict):\n",
    "    #print(df_log.iloc[-timestamp:])\n",
    "    df_log = [output_predict[:, i].astype('float32')*np.std(df.iloc[:, i+1]).astype('float32')+np.mean(df.iloc[:, i+1]).astype('float32') for i in range(df.shape[1]-1)]\n",
    "    df_log = pd.DataFrame(df_log)\n",
    "    df_log = pd.DataFrame(df_log.values.T, index=df_log.columns, columns=df_log.index)\n",
    "    df_log_norm = [output_predict[:, i].astype('float32') for i in range(df.shape[1]-1)]\n",
    "    df_log_norm = pd.DataFrame(df_log_norm)\n",
    "    df_log_norm = pd.DataFrame(df_log_norm.values.T, index=df_log_norm.columns, columns=df_log_norm.index)\n",
    "    #print(df_log)\n",
    "    #df_log = minmax.inverse_transform(output_predict)\n",
    "    date_ori = pd.Series(date_ori).dt.strftime(date_format = '%Y-%m-%d').tolist()\n",
    "    \n",
    "    return df_log, df_log_norm, date_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_data(df, df_all):\n",
    "    df_train = df\n",
    "    df = df_all\n",
    "    #print(df_log.shape[0])\n",
    "    return df_train, df\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined(df, df_log):\n",
    "    numcols=len(list(df))\n",
    "    current_palette = sns.color_palette('Paired', 2*numcols)\n",
    "    fig = plt.figure(figsize = (15, 10))\n",
    "    ax = plt.subplot(111)\n",
    "    x_range_original = np.arange(df.shape[0])\n",
    "    x_range_future = np.arange(df_log.shape[0])\n",
    "    for ind in range(numcols-1):\n",
    "        ind2=ind+1\n",
    "        ax.plot(\n",
    "            x_range_original,\n",
    "            df.iloc[:, ind2],\n",
    "            label = 'true '+'%d'%ind,\n",
    "            color = current_palette[ind*2],\n",
    "        )\n",
    "        ax.plot(\n",
    "            x_range_future,\n",
    "            anchor(df_log.iloc[:, ind], 0.5),\n",
    "            label = 'predict '+'%d'%ind,\n",
    "            color = current_palette[ind*2+1],\n",
    "        )\n",
    "    box = ax.get_position()\n",
    "    ax.set_position(\n",
    "        [box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9]\n",
    "    )\n",
    "    ax.legend(\n",
    "        loc = 'upper center',\n",
    "        bbox_to_anchor = (0.5, -0.05),\n",
    "        fancybox = True,\n",
    "        shadow = True,\n",
    "        ncol = 5,\n",
    "    )\n",
    "    plt.title('overlap stock market')\n",
    "    #plt.xticks(x_range_future[::30], date_ori[::30])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separated(df, df_log):\n",
    "    numcols=len(list(df))\n",
    "    current_palette = sns.color_palette('Paired', 2*numcols)\n",
    "    fig = plt.figure(figsize = (20, 8))\n",
    "    x_range_original = np.arange(df.shape[0])\n",
    "    x_range_future = np.arange(df_log.shape[0])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for ind in range(numcols-1):\n",
    "        ind2=ind+1\n",
    "        plt.plot(\n",
    "            x_range_original,\n",
    "            df.iloc[:, ind2],\n",
    "            label = 'true '+'%d'%ind,\n",
    "            color = current_palette[ind*2],\n",
    "        )\n",
    "    #plt.xticks(x_range_original[::60], df.iloc[:, 0].tolist()[::60])\n",
    "    plt.legend()\n",
    "    plt.title('true market')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for ind in range(numcols-1):\n",
    "        plt.plot(\n",
    "            x_range_future,\n",
    "            anchor(df_log.iloc[:, ind], 0.5),\n",
    "            label = 'predict '+'%d'%ind,\n",
    "            color = current_palette[ind*2+1],\n",
    "        )\n",
    "    #plt.xticks(x_range_future[::60], date_ori[::60])\n",
    "    plt.legend()\n",
    "    plt.title('predict market')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRMSE(X,Y): \n",
    "  return (np.linalg.norm(X-Y, ord=2)/len(Y))**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_val_train(df, df_log, num_train):\n",
    "    tmp = calculateRMSE(df.iloc[0:num_train, 0],df_log.iloc[0:num_train, 0])\n",
    "    #print(tmp)\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_val_test(df, df_log, num_train):\n",
    "    num_all = df.shape[0]\n",
    "    tmp = calculateRMSE(df.iloc[num_train:num_all, 0],df_log.iloc[num_train:num_all, 0])\n",
    "    #print(tmp)\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNameSets = []\n",
    "modelNameSets.append('naive')\n",
    "modelNameSets.append('avg')\n",
    "modelNameSets.append('moving_window')\n",
    "modelNameSets.append('exp')\n",
    "modelNameSets.append('Holt-winters')\n",
    "\n",
    "num_run = 10\n",
    "num_layers = 1\n",
    "size_layer = 10\n",
    "timestamp = 1\n",
    "epoch = 10\n",
    "dropout_rate = 0.7\n",
    "future_day = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../dataset/Stk.0941.HK.all.csv'\n",
    "\n",
    "ALL_ERR_TRAIN = []\n",
    "ALL_ERR_TEST  = []\n",
    "\n",
    "ALL_ERR_MEAN_TRAIN = []\n",
    "ALL_ERR_MEAN_TEST  = []\n",
    "\n",
    "for modelName in modelNameSets:\n",
    "\n",
    "    all_error_train = []\n",
    "    all_error_test  = []\n",
    "\n",
    "    for _ in range(num_run):\n",
    "        df, df_all, date_ori, date_all_ori, num_train = get_stock_data(filename)\n",
    "        df_log, df_all_log = data_norm(df, df_all)\n",
    "        modelnn, sess = model_init_paras(num_layers, df_log, size_layer, dropout_rate)\n",
    "        model_train(modelName, df_log, modelnn)\n",
    "        output_predict, upper_b, df_log, date_ori = model_predict(df_all_log, date_all_ori, future_day, timestamp, num_layers, size_layer)\n",
    "        df_log, date_ori, output_predict = model_predict_more(output_predict, upper_b, df_log, date_ori)\n",
    "        df_log, df_log_norm, date_ori = denorm_data(df_log, date_ori, output_predict)\n",
    "        df_train, df = swap_data(df, df_all)\n",
    "        plot_combined(df, df_log)\n",
    "        plot_separated(df, df_log)\n",
    "        error_train = get_error_val_train(df_all_log, df_log_norm, num_train)\n",
    "        error_test = get_error_val_test(df_all_log, df_log_norm, num_train)\n",
    "        if _ == 0:\n",
    "            all_error_train = error_train\n",
    "            all_error_test  = error_test\n",
    "        else:\n",
    "            all_error_train = np.vstack((all_error_train,error_train))\n",
    "            all_error_test  = np.vstack((all_error_test,error_test))\n",
    "\n",
    "    print('all train')\n",
    "    print(all_error_train)\n",
    "    print('all test')\n",
    "    print(all_error_test)\n",
    "    print('mean train')\n",
    "    print(np.mean(all_error_train, axis=0))\n",
    "    print('mean test')\n",
    "    print(np.mean(all_error_test, axis=0))\n",
    "    \n",
    "    ALL_ERR_TRAIN.append(all_error_train)\n",
    "    ALL_ERR_TEST.append(all_error_test)\n",
    "    ALL_ERR_MEAN_TRAIN.append(np.mean(all_error_train, axis=0))\n",
    "    ALL_ERR_MEAN_TEST.append(np.mean(all_error_test, axis=0))\n",
    "    \n",
    "print('all train')\n",
    "print(ALL_ERR_TRAIN)\n",
    "print('all test')\n",
    "print(ALL_ERR_TEST)\n",
    "print('all train mean')\n",
    "print(ALL_ERR_MEAN_TRAIN)\n",
    "print('all test mean')\n",
    "print(ALL_ERR_MEAN_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('all train')\n",
    "print(ALL_ERR_TRAIN)\n",
    "print('all test')\n",
    "print(ALL_ERR_TEST)\n",
    "print('all train mean')\n",
    "print(ALL_ERR_MEAN_TRAIN)\n",
    "print('all test mean')\n",
    "print(ALL_ERR_MEAN_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../dataset/Stk.1288.HK.all.csv'\n",
    "\n",
    "ALL_ERR_TRAIN = []\n",
    "ALL_ERR_TEST  = []\n",
    "\n",
    "ALL_ERR_MEAN_TRAIN = []\n",
    "ALL_ERR_MEAN_TEST  = []\n",
    "\n",
    "for modelName in modelNameSets:\n",
    "\n",
    "    all_error_train = []\n",
    "    all_error_test  = []\n",
    "\n",
    "    for _ in range(num_run):\n",
    "        df, df_all, date_ori, date_all_ori, num_train = get_stock_data(filename)\n",
    "        df_log, df_all_log = data_norm(df, df_all)\n",
    "        modelnn, sess = model_init_paras(num_layers, df_log, size_layer, dropout_rate)\n",
    "        model_train(modelName, df_log, modelnn)\n",
    "        output_predict, upper_b, df_log, date_ori = model_predict(df_all_log, date_all_ori, future_day, timestamp, num_layers, size_layer)\n",
    "        df_log, date_ori, output_predict = model_predict_more(output_predict, upper_b, df_log, date_ori)\n",
    "        df_log, df_log_norm, date_ori = denorm_data(df_log, date_ori, output_predict)\n",
    "        df_train, df = swap_data(df, df_all)\n",
    "        plot_combined(df, df_log)\n",
    "        plot_separated(df, df_log)\n",
    "        error_train = get_error_val_train(df_all_log, df_log_norm, num_train)\n",
    "        error_test = get_error_val_test(df_all_log, df_log_norm, num_train)\n",
    "        if _ == 0:\n",
    "            all_error_train = error_train\n",
    "            all_error_test  = error_test\n",
    "        else:\n",
    "            all_error_train = np.vstack((all_error_train,error_train))\n",
    "            all_error_test  = np.vstack((all_error_test,error_test))\n",
    "\n",
    "    print('all train')\n",
    "    print(all_error_train)\n",
    "    print('all test')\n",
    "    print(all_error_test)\n",
    "    print('mean train')\n",
    "    print(np.mean(all_error_train, axis=0))\n",
    "    print('mean test')\n",
    "    print(np.mean(all_error_test, axis=0))\n",
    "    \n",
    "    ALL_ERR_TRAIN.append(all_error_train)\n",
    "    ALL_ERR_TEST.append(all_error_test)\n",
    "    ALL_ERR_MEAN_TRAIN.append(np.mean(all_error_train, axis=0))\n",
    "    ALL_ERR_MEAN_TEST.append(np.mean(all_error_test, axis=0))\n",
    "    \n",
    "print('all train')\n",
    "print(ALL_ERR_TRAIN)\n",
    "print('all test')\n",
    "print(ALL_ERR_TEST)\n",
    "print('all train mean')\n",
    "print(ALL_ERR_MEAN_TRAIN)\n",
    "print('all test mean')\n",
    "print(ALL_ERR_MEAN_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('all train')\n",
    "print(ALL_ERR_TRAIN)\n",
    "print('all test')\n",
    "print(ALL_ERR_TEST)\n",
    "print('all train mean')\n",
    "print(ALL_ERR_MEAN_TRAIN)\n",
    "print('all test mean')\n",
    "print(ALL_ERR_MEAN_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../dataset/Stk.0005.HK.all.csv'\n",
    "\n",
    "ALL_ERR_TRAIN = []\n",
    "ALL_ERR_TEST  = []\n",
    "\n",
    "ALL_ERR_MEAN_TRAIN = []\n",
    "ALL_ERR_MEAN_TEST  = []\n",
    "\n",
    "for modelName in modelNameSets:\n",
    "\n",
    "    all_error_train = []\n",
    "    all_error_test  = []\n",
    "\n",
    "    for _ in range(num_run):\n",
    "        df, df_all, date_ori, date_all_ori, num_train = get_stock_data(filename)\n",
    "        df_log, df_all_log = data_norm(df, df_all)\n",
    "        modelnn, sess = model_init_paras(num_layers, df_log, size_layer, dropout_rate)\n",
    "        model_train(modelName, df_log, modelnn)\n",
    "        output_predict, upper_b, df_log, date_ori = model_predict(df_all_log, date_all_ori, future_day, timestamp, num_layers, size_layer)\n",
    "        df_log, date_ori, output_predict = model_predict_more(output_predict, upper_b, df_log, date_ori)\n",
    "        df_log, df_log_norm, date_ori = denorm_data(df_log, date_ori, output_predict)\n",
    "        df_train, df = swap_data(df, df_all)\n",
    "        plot_combined(df, df_log)\n",
    "        plot_separated(df, df_log)\n",
    "        error_train = get_error_val_train(df_all_log, df_log_norm, num_train)\n",
    "        error_test = get_error_val_test(df_all_log, df_log_norm, num_train)\n",
    "        if _ == 0:\n",
    "            all_error_train = error_train\n",
    "            all_error_test  = error_test\n",
    "        else:\n",
    "            all_error_train = np.vstack((all_error_train,error_train))\n",
    "            all_error_test  = np.vstack((all_error_test,error_test))\n",
    "\n",
    "    print('all train')\n",
    "    print(all_error_train)\n",
    "    print('all test')\n",
    "    print(all_error_test)\n",
    "    print('mean train')\n",
    "    print(np.mean(all_error_train, axis=0))\n",
    "    print('mean test')\n",
    "    print(np.mean(all_error_test, axis=0))\n",
    "    \n",
    "    ALL_ERR_TRAIN.append(all_error_train)\n",
    "    ALL_ERR_TEST.append(all_error_test)\n",
    "    ALL_ERR_MEAN_TRAIN.append(np.mean(all_error_train, axis=0))\n",
    "    ALL_ERR_MEAN_TEST.append(np.mean(all_error_test, axis=0))\n",
    "    \n",
    "print('all train')\n",
    "print(ALL_ERR_TRAIN)\n",
    "print('all test')\n",
    "print(ALL_ERR_TEST)\n",
    "print('all train mean')\n",
    "print(ALL_ERR_MEAN_TRAIN)\n",
    "print('all test mean')\n",
    "print(ALL_ERR_MEAN_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('all train')\n",
    "print(ALL_ERR_TRAIN)\n",
    "print('all test')\n",
    "print(ALL_ERR_TEST)\n",
    "print('all train mean')\n",
    "print(ALL_ERR_MEAN_TRAIN)\n",
    "print('all test mean')\n",
    "print(ALL_ERR_MEAN_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
